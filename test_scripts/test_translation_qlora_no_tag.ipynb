{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce69b81e-1902-460b-809e-e83050cb6774",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqq -U transformers datasets huggingface_hub accelerate bitsandbytes --progress-bar off\n",
    "!FLASH_ATTENTION_SKIP_CUDA_BUILD=TRUE pip install -qqq -U flash-attn --no-build-isolation pip install flash-attn --progress-bar off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00d8cbae-9484-4013-9cdd-4057653463f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d03f04fa6774a378c614d9779cec126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d821f3dc-9b0c-46ef-b916-273c13c36bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697b0abe9e9a4214a2327b19611d7aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "# Set torch dtype and attention implementation\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    !pip install -qqq flash-attn\n",
    "    torch_dtype = torch.bfloat16\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "    attn_implementation = \"eager\"\n",
    "\n",
    "base_model = \"meta-llama/Meta-Llama-3-8B\"\n",
    "new_model = \"Meta-Llama-3-8B-qlora-translation-no-tag\"\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(new_model)\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation,\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14563d80-8385-4d03-bc02-5a1d3bc4755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, interleave_datasets\n",
    "\n",
    "def get_dataset():\n",
    "    lm_datasets_train = []\n",
    "    lm_datasets_test = []\n",
    "\n",
    "    single_lang = [\"eng\", \"yue\", \"cmn\"]\n",
    "    lang_datasets = [\"eng-yue\", \"cmn-yue\"]\n",
    "    lang_pairs = [\"eng-yue\", \"yue-cmn\"]\n",
    "\n",
    "    tag_name_dict = {}\n",
    "    for lang in single_lang:\n",
    "        tag_name_dict[lang] = f'{lang}:'\n",
    "\n",
    "    prompt_examples = {}\n",
    "\n",
    "    for i, lang_dataset in enumerate(lang_datasets):\n",
    "\n",
    "        lm_dataset = load_dataset(\"AlienKevin/yue-cmn-eng\", lang_dataset)\n",
    "        lm_dataset_train = lm_dataset[\"train\"]\n",
    "        lm_dataset_train = lm_dataset_train.shuffle(seed=42)\n",
    "\n",
    "        source_lang, target_lang = lang_pairs[i].split(\"-\")\n",
    "\n",
    "        def preprocess_train(example):\n",
    "            example = example['translation']\n",
    "            return {\"input\": 'input:' + example[source_lang] + '\\n' + 'output:' + example[target_lang]}\n",
    "\n",
    "        lm_dataset_train = lm_dataset_train.select(range(10))\n",
    "        prompt_examples[lang_pairs[i]] = [example['input'] for example in lm_dataset_train.map(preprocess_train, remove_columns=['translation']).take(10)]\n",
    "\n",
    "    prompts = {pair: '\\n'.join(examples) + '\\n' for pair, examples in prompt_examples.items()}\n",
    "    print(prompts)\n",
    "    \n",
    "    for i, lang_dataset in enumerate(lang_datasets):\n",
    "\n",
    "        lm_dataset = load_dataset(\"AlienKevin/yue-cmn-eng\", lang_dataset)\n",
    "        lm_dataset_test = lm_dataset[\"test\"]\n",
    "\n",
    "        source_lang, target_lang = lang_pairs[i].split(\"-\")\n",
    "\n",
    "        def preprocess_eval(examples):\n",
    "            examples[\"inputs\"] = [prompts[lang_pairs[i]] + 'input:' + example[source_lang] + '\\n' + 'output:' for example in examples[\"translation\"]]\n",
    "            del examples['translation']\n",
    "            return examples\n",
    "        \n",
    "        lm_dataset_test = lm_dataset_test.map(preprocess_eval, batched=True)\n",
    "        lm_datasets_test.append(lm_dataset_test)\n",
    "    \n",
    "    eval_dataset = interleave_datasets(lm_datasets_test)\n",
    "    return prompts, eval_dataset, tag_name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "639b897d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f807791be74df7ba2069730de4759a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c80d94d8d58d45e88ff1e371464b06e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/82.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be12f174fcee4afda726aeedc53f800e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/624k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf492db07511451eb90b16c17aea3442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6749ea82df4dc29b134c51c62293b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/11504 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5587e3bf6cdf4768b8e8991f61c69d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eng-yue': \"input:Please don't put toilet paper into the urinal, so as to avoid clogging it, thanks for your cooperation.\\noutput:請勿將廁紙放在尿兜内，以免淤塞，多謝合作。\\ninput:This guy is very greedy for money; he was caught stealing money from his company before.\\noutput:呢條友好貪錢㗎，之前俾人發現佢偷公司錢。\\ninput:nostril\\noutput:鼻哥窿\\ninput:As your informer, I'll certainly pass on any information to you.\\noutput:我做得你條針，實會過料畀你。\\ninput:This website was designed by me.\\noutput:呢個係我自己設計嘅網站。\\ninput:to see the world\\noutput:見世面\\ninput:Mum! Are you fine?\\noutput:媽！你有冇事啊？\\ninput:I am becoming clumsier as I get older.\\noutput:我老咗做嘢係論盡啲。\\ninput:This shirt doesn't have even one pocket.\\noutput:呢件裇衫一個衫袋都冇。\\ninput:a rain shower\\noutput:一陣雨\\n\", 'yue-cmn': 'input:見衫係紅色嘅\\noutput:衣服是红色的\\ninput:嗰個地方好多時有賊劏死牛，冇乜事唔好行去嗰度\\noutput:那个地方经常有贼烂路抢劫，没什么事不要走到那儿去\\ninput:唔成功都唔使心淡吖\\noutput:不成功也不用著心灰意冷\\ninput:睇呢啲濕星嘢你要唔要呀\\noutput:看你要不要这些琐碎的东西\\ninput:叫你整闊啲，你又闊過龍\\noutput:叫你弄宽点儿，你又弄得太宽了\\ninput:我點會笑你喎，我自己都唔叻得去邊\\noutput:我怎么会笑你的，我自己也聪明不到哪里去\\ninput:呢幾張枱由我包起\\noutput:这几张桌子我包了\\ninput:我揸弗，但係即係我可能冇乜主意呢\\noutput:我拿主意，但是就是我可能没什么主义啦\\ninput:佢嘅字寫得好四正\\noutput:他的字写得很端正\\ninput:係唔係好啲呀？\\noutput:是不是好一点？\\n'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660024316307496fb7a9597424623cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82cfcb0e5e21412594df0a63a33280f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompts, eval_dataset, tag_name_dict = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eecc95da-b9bd-40dc-acab-3aed3186b9ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['inputs'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dd1a9f7-8d17-4023-984c-0ce1a4bb0dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': [\"input:Please don't put toilet paper into the urinal, so as to avoid clogging it, thanks for your cooperation.\\noutput:請勿將廁紙放在尿兜内，以免淤塞，多謝合作。\\ninput:This guy is very greedy for money; he was caught stealing money from his company before.\\noutput:呢條友好貪錢㗎，之前俾人發現佢偷公司錢。\\ninput:nostril\\noutput:鼻哥窿\\ninput:As your informer, I'll certainly pass on any information to you.\\noutput:我做得你條針，實會過料畀你。\\ninput:This website was designed by me.\\noutput:呢個係我自己設計嘅網站。\\ninput:to see the world\\noutput:見世面\\ninput:Mum! Are you fine?\\noutput:媽！你有冇事啊？\\ninput:I am becoming clumsier as I get older.\\noutput:我老咗做嘢係論盡啲。\\ninput:This shirt doesn't have even one pocket.\\noutput:呢件裇衫一個衫袋都冇。\\ninput:a rain shower\\noutput:一陣雨\\ninput:This is really amusing, a radio controlled car that can climb on walls.\\noutput:\",\n",
       "  'input:見衫係紅色嘅\\noutput:衣服是红色的\\ninput:嗰個地方好多時有賊劏死牛，冇乜事唔好行去嗰度\\noutput:那个地方经常有贼烂路抢劫，没什么事不要走到那儿去\\ninput:唔成功都唔使心淡吖\\noutput:不成功也不用著心灰意冷\\ninput:睇呢啲濕星嘢你要唔要呀\\noutput:看你要不要这些琐碎的东西\\ninput:叫你整闊啲，你又闊過龍\\noutput:叫你弄宽点儿，你又弄得太宽了\\ninput:我點會笑你喎，我自己都唔叻得去邊\\noutput:我怎么会笑你的，我自己也聪明不到哪里去\\ninput:呢幾張枱由我包起\\noutput:这几张桌子我包了\\ninput:我揸弗，但係即係我可能冇乜主意呢\\noutput:我拿主意，但是就是我可能没什么主义啦\\ninput:佢嘅字寫得好四正\\noutput:他的字写得很端正\\ninput:係唔係好啲呀？\\noutput:是不是好一点？\\ninput:做埋晒鬼五馬六嘅嘢\\noutput:']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6cccb0a-ee04-4667-858a-cf77bafd2054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/PygmalionAI/pygmalion-6b/discussions/25#64387bf26c8841ba74e7d9c0\n",
    "from transformers import StoppingCriteria\n",
    "\n",
    "class TranslationStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, prompts):\n",
    "        self.prompts = prompts\n",
    "        \n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Get the generated text as a string\n",
    "        generated_text = tokenizer.decode(input_ids[0])\n",
    "        for prompt in prompts.values():\n",
    "            generated_text = generated_text.removeprefix(prompt)\n",
    "        if generated_text.endswith('\\n'):\n",
    "            return True  # Stop generation\n",
    "        return False  # Continue generation\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1\n",
    "    \n",
    "    def __iter__(self):\n",
    "        yield self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e7ddec9-4a6e-484a-af2d-05b51b068553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [54:21<00:00,  1.09s/it]  \n"
     ]
    }
   ],
   "source": [
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "lang_tags = [\"eng:\", \"yue:\", \"cmn:\"]\n",
    "\n",
    "outputs = pipeline(\n",
    "    KeyDataset(eval_dataset, \"inputs\"),\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    stopping_criteria=TranslationStoppingCriteria(prompts),\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "def parse_translation(text):\n",
    "    lines = text.strip().split('\\n')\n",
    "    result = { 'langs': [], 'sents': [] }\n",
    "    \n",
    "    for line in lines:\n",
    "        if ':' in line:\n",
    "            lang, content = line.split(':', 1)\n",
    "            if lang in ['input', 'output']:\n",
    "                result['langs'].append('unknown')\n",
    "                result['sents'].append(content.strip())\n",
    "    \n",
    "    return result\n",
    "\n",
    "with open(f'experiment_results/translations_{new_model}.jsonl', 'w+') as f:\n",
    "    for output in tqdm(outputs, total=len(eval_dataset)):\n",
    "        generated_text = output[0]['generated_text']\n",
    "        for prompt in prompts.values():\n",
    "            generated_text = generated_text.removeprefix(prompt)\n",
    "        f.write(json.dumps(parse_translation(generated_text)) + '\\n')\n",
    "        f.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
