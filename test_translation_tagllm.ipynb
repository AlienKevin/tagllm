{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce69b81e-1902-460b-809e-e83050cb6774",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqq -U transformers datasets huggingface_hub accelerate bitsandbytes --progress-bar off\n",
    "!FLASH_ATTENTION_SKIP_CUDA_BUILD=TRUE pip install -qqq -U flash-attn --no-build-isolation pip install flash-attn --progress-bar off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00d8cbae-9484-4013-9cdd-4057653463f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5c56148bd64dd786fd9ccf8f0b3fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d821f3dc-9b0c-46ef-b916-273c13c36bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fc4ef0ca03432887fa330360adbace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "# Set torch dtype and attention implementation\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    !pip install -qqq flash-attn\n",
    "    torch_dtype = torch.bfloat16\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "    attn_implementation = \"eager\"\n",
    "\n",
    "base_model = \"meta-llama/Meta-Llama-3-8B\"\n",
    "new_model = \"Meta-Llama-3-8B-qlora-translation\"\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "num_token_per_tag = 1\n",
    "translation_tokens = [f'<|TOK{i}|>' for i in range(100, 100 + num_token_per_tag)]\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(new_model)\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation,\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14563d80-8385-4d03-bc02-5a1d3bc4755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, interleave_datasets\n",
    "\n",
    "def get_dataset(num_existing_tokens=0):\n",
    "    lm_datasets_test = []\n",
    "\n",
    "    single_lang = [\"eng\", \"yue\", \"cmn\"]\n",
    "    lang_datasets = [\"eng-yue\", \"cmn-yue\"]\n",
    "    lang_pairs = [\"eng-yue\", \"yue-cmn\"]\n",
    "\n",
    "    tag_name_dict = {}\n",
    "    for lang in single_lang:\n",
    "        tag_name_dict[lang] = \"\".join([f'<|TOK{i}|>' for i in range(num_existing_tokens, num_existing_tokens + num_token_per_tag)])\n",
    "        num_existing_tokens += num_token_per_tag\n",
    "\n",
    "    prompt_examples = {}\n",
    "\n",
    "    for i, lang_dataset in enumerate(lang_datasets):\n",
    "\n",
    "        lm_dataset = load_dataset(\"AlienKevin/yue-cmn-eng\", lang_dataset)\n",
    "        lm_dataset_train = lm_dataset[\"train\"]\n",
    "        lm_dataset_train = lm_dataset_train.shuffle(seed=42)\n",
    "\n",
    "        source_lang, target_lang = lang_pairs[i].split(\"-\")\n",
    "\n",
    "        def preprocess_train(example):\n",
    "            example = example['translation']\n",
    "            return {\"input\": tag_name_dict[source_lang] + example[source_lang] + '\\n' + tag_name_dict[target_lang] + 'translate:' + example[target_lang]}\n",
    "\n",
    "        lm_dataset_train = lm_dataset_train.select(range(10))\n",
    "        prompt_examples[lang_pairs[i]] = [example['input'] for example in lm_dataset_train.map(preprocess_train, remove_columns=['translation']).take(10)]\n",
    "\n",
    "    prompts = {pair: '\\n'.join(examples) + '\\n' for pair, examples in prompt_examples.items()}\n",
    "    print(prompts)\n",
    "    \n",
    "    for i, lang_dataset in enumerate(lang_datasets):\n",
    "\n",
    "        lm_dataset = load_dataset(\"AlienKevin/yue-cmn-eng\", lang_dataset)\n",
    "        lm_dataset_test = lm_dataset[\"test\"]\n",
    "\n",
    "        source_lang, target_lang = lang_pairs[i].split(\"-\")\n",
    "\n",
    "        def preprocess_eval(examples):\n",
    "            examples[\"inputs\"] = [prompts[lang_pairs[i]] + tag_name_dict[source_lang] + example[source_lang] + '\\n' + tag_name_dict[target_lang] + 'translate:' for example in examples[\"translation\"]]\n",
    "            del examples['translation']\n",
    "            return examples\n",
    "        \n",
    "        lm_dataset_test = lm_dataset_test.map(preprocess_eval, batched=True)\n",
    "        lm_datasets_test.append(lm_dataset_test)\n",
    "    \n",
    "    eval_dataset = interleave_datasets(lm_datasets_test)\n",
    "    return prompts, eval_dataset, tag_name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "639b897d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eng-yue': \"eng:Please don't put toilet paper into the urinal, so as to avoid clogging it, thanks for your cooperation.\\nyue:translate:請勿將廁紙放在尿兜内，以免淤塞，多謝合作。\\neng:This guy is very greedy for money; he was caught stealing money from his company before.\\nyue:translate:呢條友好貪錢㗎，之前俾人發現佢偷公司錢。\\neng:nostril\\nyue:translate:鼻哥窿\\neng:As your informer, I'll certainly pass on any information to you.\\nyue:translate:我做得你條針，實會過料畀你。\\neng:This website was designed by me.\\nyue:translate:呢個係我自己設計嘅網站。\\neng:to see the world\\nyue:translate:見世面\\neng:Mum! Are you fine?\\nyue:translate:媽！你有冇事啊？\\neng:I am becoming clumsier as I get older.\\nyue:translate:我老咗做嘢係論盡啲。\\neng:This shirt doesn't have even one pocket.\\nyue:translate:呢件裇衫一個衫袋都冇。\\neng:a rain shower\\nyue:translate:一陣雨\\n\", 'yue-cmn': 'yue:見衫係紅色嘅\\ncmn:translate:衣服是红色的\\nyue:嗰個地方好多時有賊劏死牛，冇乜事唔好行去嗰度\\ncmn:translate:那个地方经常有贼烂路抢劫，没什么事不要走到那儿去\\nyue:唔成功都唔使心淡吖\\ncmn:translate:不成功也不用著心灰意冷\\nyue:睇呢啲濕星嘢你要唔要呀\\ncmn:translate:看你要不要这些琐碎的东西\\nyue:叫你整闊啲，你又闊過龍\\ncmn:translate:叫你弄宽点儿，你又弄得太宽了\\nyue:我點會笑你喎，我自己都唔叻得去邊\\ncmn:translate:我怎么会笑你的，我自己也聪明不到哪里去\\nyue:呢幾張枱由我包起\\ncmn:translate:这几张桌子我包了\\nyue:我揸弗，但係即係我可能冇乜主意呢\\ncmn:translate:我拿主意，但是就是我可能没什么主义啦\\nyue:佢嘅字寫得好四正\\ncmn:translate:他的字写得很端正\\nyue:係唔係好啲呀？\\ncmn:translate:是不是好一点？\\n'}\n"
     ]
    }
   ],
   "source": [
    "prompts, eval_dataset, tag_name_dict = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eecc95da-b9bd-40dc-acab-3aed3186b9ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['inputs'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dd1a9f7-8d17-4023-984c-0ce1a4bb0dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': [\"eng:Please don't put toilet paper into the urinal, so as to avoid clogging it, thanks for your cooperation.\\nyue:translate:請勿將廁紙放在尿兜内，以免淤塞，多謝合作。\\neng:This guy is very greedy for money; he was caught stealing money from his company before.\\nyue:translate:呢條友好貪錢㗎，之前俾人發現佢偷公司錢。\\neng:nostril\\nyue:translate:鼻哥窿\\neng:As your informer, I'll certainly pass on any information to you.\\nyue:translate:我做得你條針，實會過料畀你。\\neng:This website was designed by me.\\nyue:translate:呢個係我自己設計嘅網站。\\neng:to see the world\\nyue:translate:見世面\\neng:Mum! Are you fine?\\nyue:translate:媽！你有冇事啊？\\neng:I am becoming clumsier as I get older.\\nyue:translate:我老咗做嘢係論盡啲。\\neng:This shirt doesn't have even one pocket.\\nyue:translate:呢件裇衫一個衫袋都冇。\\neng:a rain shower\\nyue:translate:一陣雨\\neng:This is really amusing, a radio controlled car that can climb on walls.\\nyue:translate:\",\n",
       "  'yue:見衫係紅色嘅\\ncmn:translate:衣服是红色的\\nyue:嗰個地方好多時有賊劏死牛，冇乜事唔好行去嗰度\\ncmn:translate:那个地方经常有贼烂路抢劫，没什么事不要走到那儿去\\nyue:唔成功都唔使心淡吖\\ncmn:translate:不成功也不用著心灰意冷\\nyue:睇呢啲濕星嘢你要唔要呀\\ncmn:translate:看你要不要这些琐碎的东西\\nyue:叫你整闊啲，你又闊過龍\\ncmn:translate:叫你弄宽点儿，你又弄得太宽了\\nyue:我點會笑你喎，我自己都唔叻得去邊\\ncmn:translate:我怎么会笑你的，我自己也聪明不到哪里去\\nyue:呢幾張枱由我包起\\ncmn:translate:这几张桌子我包了\\nyue:我揸弗，但係即係我可能冇乜主意呢\\ncmn:translate:我拿主意，但是就是我可能没什么主义啦\\nyue:佢嘅字寫得好四正\\ncmn:translate:他的字写得很端正\\nyue:係唔係好啲呀？\\ncmn:translate:是不是好一点？\\nyue:做埋晒鬼五馬六嘅嘢\\ncmn:translate:']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6cccb0a-ee04-4667-858a-cf77bafd2054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/PygmalionAI/pygmalion-6b/discussions/25#64387bf26c8841ba74e7d9c0\n",
    "from transformers import StoppingCriteria\n",
    "\n",
    "class TranslationStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, prompts):\n",
    "        self.prompts = prompts\n",
    "        \n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Get the generated text as a string\n",
    "        generated_text = tokenizer.decode(input_ids[0])\n",
    "        for prompt in prompts.values():\n",
    "            generated_text = generated_text.removeprefix(prompt)\n",
    "        if generated_text.endswith('\\n'):\n",
    "            return True  # Stop generation\n",
    "        return False  # Continue generation\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1\n",
    "    \n",
    "    def __iter__(self):\n",
    "        yield self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7ddec9-4a6e-484a-af2d-05b51b068553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 96/3000 [01:58<54:50,  1.13s/it]  "
     ]
    }
   ],
   "source": [
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "outputs = pipeline(\n",
    "    KeyDataset(eval_dataset, \"inputs\"),\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    stopping_criteria=TranslationStoppingCriteria(prompts),\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "def parse_translation(text):\n",
    "    lines = text.strip().split('\\n')\n",
    "    result = { 'langs': [], 'sents': [] }\n",
    "    \n",
    "    for line in lines:\n",
    "        if len(line.strip()) > 0:\n",
    "            for lang_tags in tag_name_dict.values():\n",
    "                if lang_tags in line:\n",
    "                    lang = next(lang for lang, tags in tag_name_dict.items() if tags == lang_tags)\n",
    "                    content = line.removeprefix(lang_tags)\n",
    "                    result['langs'].append(lang)\n",
    "                    result['sents'].append(content.strip().removeprefix(''.join(translation_tokens)))\n",
    "                    break\n",
    "    \n",
    "    return result\n",
    "\n",
    "with open(f'translations_{new_model}.jsonl', 'w+') as f:\n",
    "    for output in tqdm(outputs, total=len(eval_dataset)):\n",
    "        generated_text = output[0]['generated_text']\n",
    "        for prompt in prompts.values():\n",
    "            generated_text = generated_text.removeprefix(prompt)\n",
    "        f.write(json.dumps(parse_translation(generated_text)) + '\\n')\n",
    "        f.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
